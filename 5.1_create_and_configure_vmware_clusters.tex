\subsection{Create and Configure VMware Clusters}

EVC (enhanced vMotion compatibility) allows hosts in a cluster with different
CPU families to use the common denominator of CPU features. When enabled on a
cluster, only compatible hosts may be added to the cluster. Even with EVC,
you cannot mix AMD and Intel in the same cluster.

As the number and utilization of VMs grows it tends to grow disproportionately.
DRS ensures that a VM gets the resources it needs. DRS does not try to
achieve load balancing.

DRS understands the resources of the virtual infrastructure:

\begin{itemize}
\item CPU
\item memory
\item power
\item storage
\end{itemize}

There can be up to 32 ESXi hosts in a cluster.\\

Host and cluster provide resource and VMs consume resources.\\

Goals of DRS:

\begin{itemize}
\item prevent one VM from monopolizing resources
\item guarantee service levels
\item efficiently use server hardware
\item make VMware administrator's life easier
\end{itemize}

DRS is enabled at the cluster level.\\

DRS automation levels:

\begin{description}

\item[manual]
gives suggestions it that must be done manually

\item[partially automated]
VMs automatically placed on power-on, gives suggestions after that

\item[fully automated]
migration threshold is set and vCenter moves VMs automatically

\end{description}

The cluster can configured to allow individual VMs to override the cluster
setting. If this is enabled, VMs can choose any of the automation levels
or disabled.\\

Migration rules can be used to VMs together or apart.\\

DR requirements:

\begin{itemize}
\item vSphere Enterprise or Enterprise Plus licenses
\item shared storage between hosts and all VMs must be on shared storage
\item DRS will use vMotion so it vMotion needs to work between hosts
\item beware of CPU compatibility issues, consider EVC
\end{itemize}

DPM (distributed power management) will consolidate VMs onto fewer hosts when
the load is low and power off unused hosts. DPM requires DRS.

Different way to control resources:

\begin{description}

\item[reservation]
reserve a certain amount of CPU or memory

\item[limit]
set an upper bound for resources

\item[shares]
gives priority shares of CPU or memory resources, set high, normal or low
with 4:2:1 ratio or use a custom ratio, you can also set shares for IOPS
at the VM level that are used for SIOC and SDRS

\end{description}

Resource pools and vApps have an ``expandable reservation'' option. When
enabled, if the combined reservations of the virtual machines are larger
than the reservation of the resource pool or vApp, it can use resources from
its parent or ancestor.

\subsubsection{Describe DRS virtual machine entitlement}

\subsubsection{Create/Delete a DRS/HA Cluster}

HA protects against server failure by restarting virtual machines on other
hosts within the cluster. It also protects against guest OS and application
failure by monitoring a VM and resetting it if a failure occurs.\\

VM monitoring options:

\begin{itemize}
\item disabled
\item VM monitoring only
\item VM and application monitoring
\end{itemize}

If the host a critical server is on fails, it will be brought back up by HA
in the time it takes to boot. HA reduces downtime.\\

HA provides uniform protection for all guest operating systems and
applications.

When combined with DRS, HA can provide smart failover and select the best
ESXi host.

When a host is added to a vSphere cluster an agent is put on it to
communicate with agents on other hosts. HA uses TCP and UDP port 8182 for
agent to agent communication. It uses the vpxuser account. The agent can
be added to auto deployed hosts using an image profile.\\

A single host is elected as master by all hosts. The full name of the master
host is the Fault Domain Manager Master (FDMS). The host that mounts the most
datastores has an advantage in the election.\\

The master:

\begin{itemize}
\item monitors slave hosts
\item monitors the power states of all protected VMs and restart them if needed
\item manages a list of cluster hosts and protected VMs
\item is vCenter Server's interface to the cluster
\end{itemize}

The master distinguishes between failed hosts and network-isolated hosts using
datastore heartbeats.\\

Slaves report their VM state updates to master.\\

A master host commits to protecting a VM when it observes the power state
changing to on in response to a user action. The master locks a file on the
VM's datastore to claim responsibility for restarting it.\\

Types of host failure:

\begin{itemize}
\item host fails
\item host becomes network isolated
\item host loses connection to master
\end{itemize}

When master stops receiving heartbeats from slaves, it checks the datastore
heartbeat and pings the management IP. If the datastore heartbeat is ok, the
master assumes a network partition.\\

There is a preferred set of datastores for heartbeating. The default number
of heartbeat datastores is two.\\

When a host sees no HA traffic it pings the cluster isolation address. If it
cannot reach it, it declares itself isolated from the network. Host isolation
responses require that Host Monitoring Status is enabled.

Host isolation response options:

\begin{itemize}
\item leave powered on (default)
\item power off
\item shutdown
\item disabled
\end{itemize}

This option can be set on individual VMs.\\

During a network partition, vCenter Server will power on VMs but they are
only protected if they are on the same partition as the master that is
responsible for them. Also host configuration changes might not propagate to
all hosts.

During a split brain scenario VMs might be running on multiple hosts at the
same time.\\

Requirements for HA:

\begin{itemize}
\item shared storage for VMs running in the HA cluster
\item all hosts should have access to all VM networks
\item can use DRS with HA or HA only
\item all hosts must be licensed for HA
\item must have created a cluster with HA enabled
\item all hosts must have a static ip
\item at least 2 hosts in the cluster
\item at least one management network (best practice says two)
\end{itemize}

HA best practices:

\begin{itemize}

\item monitor cluster validity

\item recommended to disable host monitoring when making changes to the
network or distirbuted virtual switches

\item all networks and VMs on HA clusters must have compatible networks

\item by default the isolation IP is the default gateway but you can configure
others by setting the dsa.isolationaddressX setting, where X can be a value 1
- 10

\item use network redundancy between ESXi servers

\item always resource 25\% of capacity for HA
\end{itemize}

HA Changes for vSphere 5:

\begin{itemize}

\item no more primary / secondary, now master / slave
\item one ESXi host is master
\item FDM - fault domain manager
\item failure detection using management network and storage communications
\item one log file per server /etc/opt/vmware/fdm
\item no reliance on DNS
\item eliminate common issues
\item IPv6 support
\item enhanced UI
\item enhanced deployment

\end{itemize}

\subsubsection{Add/Remove ESXi Hosts from a DRS/HA Cluster}

When adding an ESXi host to e DRS/HA cluster you specify the hostname or
IP address, username and password.

\subsubsection{Add/Remove virtual machines from a DRS/HA Cluster}

\subsubsection{Configure Storage DRS}

Storage DRS load balances VMs based on datastores. It can be used to profile
storage and attach SLAs to VMs. Storage DRS requires a vSphere Enterprise
Plus license.\\

Without storage DRS, when you create a VM you have to manually find a
datastore with enough space and low enough latency. From there, you hope for
the best, periodically checking latency and trying to monitor for low space.
This is inefficent, costs all parties invovled time and money and provides
as opportunity for outages and slowdowns.\\

Storage DRS is enabled with Storage vMotion.\\

Storage DRS can determine initial VM placement and perform automated load
balancing based on:

\begin{itemize}

\item space utilization
\item lowest latency

\end{itemize}

Initial placement is calculated when the VM is created, when the VM is cloned
or when the VM is relocated.\\

VMs are automatically balanced by being storage vMotioned to a better
datastore if they arent receiving the latency they need.\\

Per-VM settings can override the SDRS automation level set on the datastore
cluster.\\

Storage DRS affinity rules can be based on:

\begin{description}

\item[VMDK affinity]
keep a VM's VMDKs on the same datastore, maximizes the availability when all
VMDKs are needed to run, on by default for all VMs

\item[VMDK anti-affinity]
keep a VM's VMDKs on different datastores, useful for separating log and data
disks of database VMs, can select all or a subset of a the VM's disks

\item[VM anti-affinity]
keep VMs on different datastores, similar to DRS anti-affinity rules, maximize
availbility of redundant VMs

\end{description}

Datastore clusters are cluster of datastores like DRS clusters are clusters
of hosts. When SDRS is not turned on a datastore cluster acts as a folder of
datastores.\\

When a datastore is put into maintenance mode all registered VMDKs on it are
evacuated. Templates, ISOs and unregistered VMs remain.\\

Space utilization is continuously collected and the default threshold for
action is 80\% full.\\

I/O latency is evaluated every 8 hours based on the last 24-hour trend. The
default threshold for action is 15ms.\\

Storage DRS does a cost/benefit analysis to determine if moving a VM to a
different datastore is worth it.\\

SDRS works with SIOC and takes into account which VMs have higher I/O shares.\\

It takes 16 hours of I/O stats before SDRS will make its first recommendation.\\

Schedule tasks can modify SDRS settings (such as disabling it during a backup
period).

\subsubsection{Configure Enhanced vMotion Compatibility}

\subsubsection{Monitor a DRS/HA Cluster}

\subsubsection{Configure migration thresholds for DRS and virtual machines}

\subsubsection{Configure automation levels for DRS and virtual machines}

\subsubsection{Create VM-Host and VM-VM affinity rules}

\subsubsection{Enable/Disable Host Monitoring}

\subsubsection{Enable/Configure/Disable virtual machine and application monitoring}

\subsubsection{Configure admission control for HA and virtual machines}

\begin{description}

\item[host]
make sure a host has sufficient resources to satisfy the reservations of the
VMs running on it

\item[resource pool]
make sure a resource pool can satisfy reservations, shares and limits of its
VMs

\item[vSphere HA]
make sure the cluster has sufficient resources for virtual machines recovery
if a host fails

\end{description}

Admission Control could prevent:

\begin{itemize}
\item powering on a VM
\item migrating a VM to a host, cluster or resource pool
\item increasing a CPU of memory reservation (this is the only check that can
be disabled)
\end{itemize}

Slot size is a logical representation of memory and CPU resources. It is sized
to satisfy the requirements for any powered-on VM in the cluster.\\

Failover capacity admission control:

\begin{enumerate}
\item calculate slot size
\item determine how many slots each host can hold
\item determine cluster failover capacity
\item if current failover capacity is less than configured failover capacity,
disallow the operation
\end{enumerate}

CPU slot size is the largest CPU reservation or 32 MHz if there are no
reservations.\\

Memory slot size is the largest memory reservation plus memory overhead.\\

Advanced options can be used to specify an upper bound on these.\\

The number of VMs that a host can support is the smaller of CPU slots and
memory slots.\\

The current failover capacity is the number of hosts that can
fail and leave enough slots to satisfy the requirements of all powered-on
VMs.

Host failures cluster tolerates policy can be too conservative in
heterogeneous environments because it only considers the largest VM and assumes
the largest host fails.\\

Cluster resources reserved admission control:

\begin{enumerate}

\item calculate total resource requirements for all powered-on VMs in the
cluster: sum of memory reservations plus overhead and CPU reservations of
powered-on virtual machines

\item calculate total host resources available for VMs

\item calculate current CPU and memory failover capacity for the cluster:
(total resources - required resources) / total resources

\item if current failover capacity is less than configured failover capacity,
disallow the operation

\end{enumerate}

The percentage of cluster resources policy does not address fragmentation.\\

Failover hosts admission control:\\

Hosts can be designated as failover hosts. Failover hosts will be used to
restart VMs first. VMs cannot be powered on on or migrated to failover hosts.\\

VM Restart Priority Options:

\begin{itemize}
\item disable (the VM is not restarted)
\item low
\item medium (default)
\item high
\end{itemize}

Agent virtual machines (vShield applicances, etc.) and FT secondary virtual
machines will be restarted with priority than high-priority VMs.\\

VM guest and application can be monitored by checking for heartbeats from
VMware tools and for I/O. Application monitoring requires the appropriate SDK.

\subsubsection{Determine appropriate failover methodology and required resources for an HA implementation}
